#!/usr/bin/env -S python
# -*- coding: utf-8 -*-

"""
dirdiff [OPTIONS...] dir1 dir2 [dir3...]
modified files:
1 column
moved/renamed files:
2 columns
additional files:
1 column


dirdiff [OPTIONS] --persist-lookup-tables dir1 [dir2...]
... transfer resulting `dir1.dirstate` file to another host ...
dirdiff [OPTIONS] dir3 dir1.dirstate [dir4...]
"""

import argparse
import dataclasses
import hashlib
import json
import logging
import os
import pickle
import platform
import stat
import sys
import time
from collections import defaultdict
from contextlib import contextmanager
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Annotated

LOG = logging.getLogger()
LOG.setLevel("INFO")
formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
ch = logging.StreamHandler()
ch.setFormatter(formatter)
LOG.addHandler(ch)

HOSTNAME: str = platform.node()
TIMESTAMP: int = int(time.time())


# types
@dataclasses.dataclass
class DirDiffFile:
    rootdir: Path
    position: Path
    hexdigest: str

    def __init__(self, rootdir: Path, path: Path, machine=HOSTNAME):
        self.rootdir = rootdir
        self.position = path
        self.hexdigest = file_hexdigest(self.absolute())
        self.machine = machine
        self.time = TIMESTAMP

    def absolute(self, realtime=True):
        if realtime:
            return self.rootdir / self.position
        else:
            dt = datetime.utcfromtimestamp(self.time).isoformat()
            return f"{dt} - {self.machine}::{self.rootdir}/{self.position}"


# this "Annotated" stuff allows us to document individual variables,
# see https://stackoverflow.com/a/8820636
HexDigest = Annotated[str, "file content hash type"]
MatchResults = Annotated[
    Dict[HexDigest, List[DirDiffFile]], "type for file lookup by content hash"
]
PathDiffLookup = Annotated[
    Dict[Path, List[DirDiffFile]], "type for file lookup by path"
]
HashDiffLookup = Annotated[
    Dict[HexDigest, PathDiffLookup], "type for file lookup by content hash, then path"
]


class JsonEncoder(json.JSONEncoder):
    def default(self, o):
        if dataclasses.is_dataclass(o):
            d = dataclasses.asdict(o)
            d["path"] = o.absolute()
            return d
        if isinstance(o, Path):
            return str(o)
        return super().default(o)


def parse_args():
    parser = argparse.ArgumentParser(
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "-d",
        "--debug",
        action="store_true",
        help="enable debug logging information",
    )
    parser.add_argument(
        "--ignore-modified",
        action="store_true",
        help="don't show files that were modified",
    )
    parser.add_argument(
        "--ignore-moved",
        action="store_true",
        help="don't show files that were moved around",
    )
    parser.add_argument(
        "--ignore-additional",
        action="store_true",
        help="don't show files if they weren't either moved or modified",
    )
    parser.add_argument(
        "--persist-lookup-tables",
        action="store_true",
        help="don't actually run the diff algorithm, instead save the lookup data to disk for later processing",
    )
    output_format = parser.add_mutually_exclusive_group()
    output_format.add_argument(
        "--json",
        action="store_true",
        help="display output as JSON",
    )
    parser.add_argument(
        "dirpath",
        type=Path,
        nargs="+",
        help="dir path",
    )
    args = parser.parse_args()
    if args.debug:
        LOG.setLevel(logging.DEBUG)
    return args


@contextmanager
def chdir(dirname):
    oldpwd = Path().absolute()
    try:
        os.chdir(dirname)
        yield
    finally:
        os.chdir(oldpwd)


def file_hexdigest(filepath: Path) -> str:
    hasher = hashlib.sha1()
    BUFSIZ = 65536
    try:
        with open(filepath, "rb") as f:
            buf = f.read(BUFSIZ)
            while len(buf) > 0:
                hasher.update(buf)
                buf = f.read(BUFSIZ)
    except FileNotFoundError as e:
        # if this is a symlink
        if stat.S_ISLNK(os.lstat(filepath).st_mode):
            hasher.update(os.readlink(filepath).encode())
        else:
            raise
    except OSError as e:
        if e.errno == 102:
            hasher.update(b"")
        else:
            raise
    return hasher.hexdigest()


def list_files_recursively(dirpath: Path) -> List[Path]:
    return [
        dirpath / filepath for filepath in dirpath.rglob("*") if not filepath.is_dir()
    ]


def preprocess_into_datastructs(
    rootdirs: List[Path],
) -> Tuple[HashDiffLookup, PathDiffLookup, bool]:
    """
    collect all the files from all the rootdirs and build our base data
    structures. We want to be able to look files up:
     - by content hash
     - by relative path

    rootdirs: list of rootdirs to analyze. A rootdir can either be a directory
              or a dirdiff state file created during a previous invocation
    return
        - a dict for looking up by hash
        - a dict for looking up by path
        - whether the directory was analyzed in real time or whether at least
          one of the rootdirs was loaded from a state file
    """
    hashdiff_data: HashDiffLookup = defaultdict(lambda: defaultdict(list))
    pathdiff_data: PathDiffLookup = defaultdict(list)
    realtime: bool = True

    for rootdir in rootdirs:
        statdata = os.stat(rootdir)
        # if it's a directory, analyze it
        if stat.S_ISDIR(statdata.st_mode):
            rootdir_path = Path(rootdir)
            # we want paths relative to rootdir, not absolute paths. Abstract
            # away the absolute paths by changing directory to rootdir before
            # listing files
            with chdir(rootdir_path):
                filelist = list_files_recursively(Path("."))
            for f in filelist:
                try:
                    ddf = DirDiffFile(rootdir_path, f)
                    hashdiff_data[ddf.hexdigest][ddf.position].append(ddf)
                    pathdiff_data[ddf.position].append(ddf)
                except PermissionError as e:
                    LOG.warning(f"error processing {f}: {e}")
        # if it's a state file, load it
        elif stat.S_ISREG(statdata.st_mode):
            # we're using a dump created by a previous invocation of this script
            # with --persist_lookup_tables flag
            realtime = False

            savedstate_hashdiff_data: Dict[HexDigest, Dict[Path, List[DirDiffFile]]]
            savedstate_pathdiff_data: Dict[Path, List[DirDiffFile]]
            (
                savedstate_hostname,
                savedstate_ts,
                savedstate_hashdiff_data,
                savedstate_pathdiff_data,
            ) = load_state(rootdir)

            for hd, data in savedstate_hashdiff_data.items():
                for path, dirdiff_filelist in data.items():
                    hashdiff_data[hd][path].extend(dirdiff_filelist)
            for path, dirdiff_filelist in savedstate_pathdiff_data.items():
                pathdiff_data[path].extend(dirdiff_filelist)
        else:
            raise RuntimeError(
                "dirdiff only supports directories and dirdiff_state (python pickle) files"
            )

    return hashdiff_data, pathdiff_data, realtime


def save_state(
    dirpaths: List[str], hashdiff_data: HashDiffLookup, pathdiff_data: PathDiffLookup
) -> None:
    """
    After analyzing a directory, save the rootdir state to a file specified by
    the user, for later comparison against another rootdir. This comparison can
    potentially happen on a different computer, or against the same directory
    but after some time has passed.

    todo replace dirpath with output filepath and save dirpaths as part of the
    data
    """
    savestate_filename: str = "{}.dirdiff".format(
        "_".join([os.path.basename(d) for d in dirpaths])
    )
    with open(savestate_filename, "wb") as f:
        hashdiff_data_pickleable = {k: dict(v) for k, v in hashdiff_data.items()}
        pickle.dump((HOSTNAME, TIMESTAMP, hashdiff_data_pickleable, pathdiff_data), f)


def load_state(
    dirdiff_state_path: Path,
) -> Tuple[str, int, HashDiffLookup, PathDiffLookup]:
    """
    load a previous rootdir save state, so that it can be compared with a
    different rootdir

    dirdiff_state_path: path to a save state file (pickle format) previously
                        generated by dirdiff

    return
        - hostname on which save state was created
        - timestamp at which it was created
        - hashdifflookup for looking up files by hash and path
        - pathdifflookup for looking up files by path
    """
    try:
        with open(dirdiff_state_path, "rb") as f:
            (
                dirdiff_state_hostname,
                dirdiff_state_ts,
                hashdiff_data,
                pathdiff_data,
            ) = pickle.load(f)
    except (pickle.PickleError, ValueError) as e:
        LOG.fatal(f"{dirdiff_state_path} doesn't have the appropriate format")
        sys.exit(1)

    return dirdiff_state_hostname, dirdiff_state_ts, hashdiff_data, pathdiff_data


def partition_matched_unmatched(
    hashdiff_data: HashDiffLookup, rootdirs: List[Path]
) -> Tuple[MatchResults, MatchResults]:
    """
    After building an in-memory representation of each rootdir, attempt to match
    as many files as possible. Any file that has same filepath and hash that
    exists in all rootdirs is considered matched. Any other file is considered
    unmatched.

    hashdiff_data: lookup files by hash, dict-type. Key is file hash, value is
                   itself a dict, where each key is a relative path (relative to
                   rootdirs) and each value is a list of files. This list of
                   files should all match both hash and relative path
    rootdirs: list of rootdirs being examined

    return 2 MatchResult objects (file lookup by content hash
           -> dict[hash, list of files]). 1st one is matched files, second one
           is unmatched files
    """
    # key is the file hexdigest, value is list of dirdiffobjects, each
    # representing a file with that hash
    matched: MatchResults = defaultdict(list)
    unmatched: MatchResults = defaultdict(list)

    # dir1/foopath1/barfile1 is matched with dir2/foopath2/barfile2 if:
    #    - "foopath1" == "foopath2" and "barfile1" == "barfile2"
    #    - hash(barfile1) == hash(barfile2).
    # Otherwise, barfile1 is unmatched.
    # For 3+ rootdirs, all filepaths and hashes have to match, and the file must
    # be present in all rootdirs
    len_rootdirs = len(rootdirs)
    for hexdigest, filedata in hashdiff_data.items():
        for _, filelist in filedata.items():
            # if all rootdirs have the same DirdiffFile -> we have a match
            if len(filelist) == len_rootdirs:
                # make sure no bugs in input data: each DirDiffFile has to have:
                # - one file per time/machine/rootdir combo
                # - same hash
                # - same relative path within each separate rootdir
                # For this check, create sets and ensure each set has the
                # expected length
                assert (
                    len({(f.time, f.machine, f.rootdir) for f in filelist})
                    == len_rootdirs
                    and len({f.hexdigest for f in filelist}) == 1
                    and len({f.position for f in filelist}) == 1
                ), ("matching error: %s" % filelist)
                matched[hexdigest].extend(filelist)
            # else we don't have a match
            else:
                for f in filelist:
                    unmatched[f.hexdigest].append(f)

    return matched, unmatched


def partition_unmatched_by_type(
    unmatched: MatchResults,
    pathdiff_data: PathDiffLookup,
    rootdirs: List[Path],
) -> Tuple[PathDiffLookup, List[List[DirDiffFile]], List[DirDiffFile]]:
    """
    Categorize each file into one of modified, moved/renamed, or additional

    unmatched: unmatched files as a dict. key is file hash, value is list of
               files with that hash
    pathdiff_data: file lookup by path. Dict where key is path relative to
                   rootdir, value is list of files with that relative path (each
                   rootdir might have its own file)
    rootdirs: list of directories we're comparing against one another
    """
    # We distinguish 3 cases for unmatched files:
    #  - modified file
    #  - moved/renamed file
    #  - additional file
    #
    # dir1/foopath1/barfile1 was modified if:
    #  - dir2/foopath2/barfile2 with foopath1 == foopath2 exists
    #  - hash(barfile1) != hash(barfile2)
    # For these files, the invariant is the file path -> dict of paths that each
    # map to DirdiffFile list.
    #
    # dir1/foopath1/barfile1 was moved/renamed if:
    #  - there exists dir2/foopath2/barfile2 such that foopath1 != foopath2 and
    #    hash(barfile1) == hash(barfile2)
    #  - neither barfile1 nor barfile2 are modified as per the definition above
    # We will want to
    #
    # dir1/foopath1/barfile1 is additional if:
    #  - barfile1 is not modified as per the definition above
    #  - barfile1 is not moved/renamed as per the definition above
    len_rootdirs = len(rootdirs)
    modified: PathDiffLookup = defaultdict(list)
    moved: List[List[DirDiffFile]] = []
    additional: List[DirDiffFile] = []

    for _, filelist in unmatched.items():
        unmodified = []
        for f in filelist:
            # if all rootdirs have a file at this position -> file was modified
            if len(pathdiff_data[f.position]) == len_rootdirs:
                modified[f.position].append(f)
            else:
                unmodified.append(f)
        # any unmodified file is either moved/renamed, or isn't in all rootdirs
        # all these files have the same hexdigest, and don't necessarily have
        # the same path
        if len(unmodified) > 0:
            # if there's one file per repo, then at least one file was moved
            if len({f.rootdir for f in unmodified}) == len_rootdirs:
                moved.append(unmodified)
            # otherwise, who knows
            else:
                additional.extend(unmodified)

    return modified, moved, additional


def print_results(
    modified: PathDiffLookup,
    moved: List[List[DirDiffFile]],
    additional: List[DirDiffFile],
    realtime: bool = True,
    ignore_modified: bool = False,
    ignore_moved: bool = False,
    ignore_additional: bool = False,
    json_output: bool = False,
) -> None:
    output_dict = {}
    if not ignore_modified:
        output_dict["modified"] = {str(k): v for k, v in modified.items()}
    if not ignore_moved:
        output_dict["moved"] = moved
    if not ignore_additional:
        output_dict["added"] = additional

    if json_output:
        print(json.dumps(output_dict, indent=2, cls=JsonEncoder))
    else:
        if not ignore_modified:
            for filepath in output_dict["modified"]:
                print("modified:", filepath)
        if not ignore_moved:
            for ren in output_dict["moved"]:
                moved_str = "\t".join([str(f.absolute(realtime=realtime)) for f in ren])
                print("moved/renamed:", moved_str)
        if not ignore_additional:
            for add in output_dict["added"]:
                print("added:", add.absolute(realtime=realtime))


def main():
    args = parse_args()

    hashdiff_data, pathdiff_data, realtime = preprocess_into_datastructs(args.dirpath)
    if args.persist_lookup_tables:
        save_state(args.dirpath, hashdiff_data, pathdiff_data)
    else:
        matched, unmatched = partition_matched_unmatched(hashdiff_data, args.dirpath)
        modified, moved, additional = partition_unmatched_by_type(
            unmatched, pathdiff_data, args.dirpath
        )
        print_results(
            modified,
            moved,
            additional,
            realtime=realtime,
            ignore_modified=args.ignore_modified,
            ignore_moved=args.ignore_moved,
            ignore_additional=args.ignore_additional,
            json_output=args.json,
        )


if __name__ == "__main__":
    main()
