#!/usr/bin/env -S python
# -*- coding: utf-8 -*-

"""
dirdiff [OPTIONS...] dir1 dir2
modified files:
1 column
moved/renamed files:
2 columns
additional files:
1 column
"""

import argparse
import dataclasses
import hashlib
import json
import logging
import os
import pickle
import platform
import stat
import sys
import time
from collections import defaultdict
from contextlib import contextmanager
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple

LOG = logging.getLogger()
LOG.setLevel("INFO")
formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
ch = logging.StreamHandler()
ch.setFormatter(formatter)
LOG.addHandler(ch)

HOSTNAME: str = platform.node()
TIMESTAMP: int = int(time.time())


# types
@dataclasses.dataclass
class DirDiffFile:
    rootdir: Path
    position: Path
    hexdigest: str

    def __init__(self, rootdir: Path, path: Path, machine=HOSTNAME):
        self.rootdir = rootdir
        self.position = path
        self.hexdigest = file_hexdigest(self.absolute())
        self.machine = machine
        self.time = TIMESTAMP

    def absolute(self, realtime=True):
        if realtime:
            return self.rootdir / self.position
        else:
            dt = datetime.utcfromtimestamp(self.time).isoformat()
            return f"{dt} - {self.machine}::{self.rootdir}/{self.position}"


HexDigest = str
HashDiffLookup = Dict[HexDigest, Dict[Path, List[DirDiffFile]]]
PathDiffLookup = Dict[Path, List[DirDiffFile]]
MatchResults = Dict[HexDigest, List[DirDiffFile]]


class JsonEncoder(json.JSONEncoder):
    def default(self, o):
        if dataclasses.is_dataclass(o):
            d = dataclasses.asdict(o)
            d["path"] = o.absolute()
            return d
        if isinstance(o, Path):
            return str(o)
        return super().default(o)


def parse_args():
    parser = argparse.ArgumentParser(
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "-d",
        "--debug",
        action="store_true",
        help="enable debug logging information",
    )
    parser.add_argument(
        "--ignore-modified",
        action="store_true",
        help="don't show files that were modified",
    )
    parser.add_argument(
        "--ignore-moved",
        action="store_true",
        help="don't show files that were moved around",
    )
    parser.add_argument(
        "--ignore-additional",
        action="store_true",
        help="don't show files if they weren't either moved or modified",
    )
    parser.add_argument(
        "--persist-lookup-tables",
        action="store_true",
        help="don't actually run the diff algorithm, instead save the lookup data to disk for later processing",
    )
    output_format = parser.add_mutually_exclusive_group()
    output_format.add_argument(
        "--json",
        action="store_true",
        help="display output as JSON",
    )
    parser.add_argument(
        "dirpath",
        type=Path,
        nargs="+",
        help="dir path",
    )
    args = parser.parse_args()
    if args.debug:
        LOG.setLevel(logging.DEBUG)
    return args


@contextmanager
def chdir(dirname):
    oldpwd = Path().absolute()
    try:
        os.chdir(dirname)
        yield
    finally:
        os.chdir(oldpwd)


def apply_to_dir(dirpath, func):
    return [
        dirpath / func(filepath)
        for filepath in Path(dirpath).rglob("*")
        if not filepath.is_dir()
    ]


def file_hexdigest(filepath) -> str:
    hasher = hashlib.sha1()
    BUFSIZ = 65536
    try:
        with open(filepath, "rb") as f:
            buf = f.read(BUFSIZ)
            while len(buf) > 0:
                hasher.update(buf)
                buf = f.read(BUFSIZ)
    except FileNotFoundError as e:
        # if this is a symlink
        if stat.S_ISLNK(os.lstat(filepath).st_mode):
            hasher.update(b"")
        else:
            raise
    except OSError as e:
        if e.errno == 102:
            hasher.update(b"")
        else:
            raise
    return hasher.hexdigest()


def preprocess_into_datastructs(
    rootdirs: List[Path],
) -> Tuple[HashDiffLookup, PathDiffLookup, bool]:
    # let's collect all the files from all the rootdirs and build our base data structures
    hashdiff_data: HashDiffLookup = defaultdict(lambda: defaultdict(list))
    pathdiff_data: PathDiffLookup = defaultdict(list)
    realtime: bool = True

    for rootdir in rootdirs:
        statdata = os.stat(rootdir)
        if stat.S_ISDIR(statdata.st_mode):
            rootdir_path = Path(rootdir)
            with chdir(rootdir_path):
                filelist = apply_to_dir(".", lambda x: x)
            for f in filelist:
                try:
                    ddf = DirDiffFile(rootdir_path, f)
                    hashdiff_data[ddf.hexdigest][ddf.position].append(ddf)
                    pathdiff_data[ddf.position].append(ddf)
                except PermissionError as e:
                    LOG.warning(f"error processing {f}: {e}")
        elif stat.S_ISREG(statdata.st_mode):
            savedstate_hashdiff_data: Dict[HexDigest, Dict[Path, List[DirDiffFile]]]
            savedstate_pathdiff_data: Dict[Path, List[DirDiffFile]]
            (
                savedstate_hostname,
                savedstate_ts,
                savedstate_hashdiff_data,
                savedstate_pathdiff_data,
            ) = load_state(rootdir)
            realtime = (
                realtime
                and savedstate_hostname == HOSTNAME
                and savedstate_ts == TIMESTAMP
            )

            for hd, data in savedstate_hashdiff_data.items():
                for path, dirdiff_filelist in data.items():
                    hashdiff_data[hd][path].extend(dirdiff_filelist)
            for path, dirdiff_filelist in savedstate_pathdiff_data.items():
                pathdiff_data[path].extend(dirdiff_filelist)
        else:
            raise RuntimeError(
                "dirdiff only supports directories and dirdiff_state (python pickle) files"
            )

    return hashdiff_data, pathdiff_data, realtime


def save_state(
    dirpaths: List[str], hashdiff_data: HashDiffLookup, pathdiff_data: PathDiffLookup
) -> None:
    savestate_filename: str = "{}.dirdiff".format(
        "_".join([os.path.basename(d) for d in dirpaths])
    )
    with open(savestate_filename, "wb") as f:
        hashdiff_data_pickleable = {k: dict(v) for k, v in hashdiff_data.items()}
        pickle.dump((HOSTNAME, TIMESTAMP, hashdiff_data_pickleable, pathdiff_data), f)


def load_state(
    dirdiff_state_path: Path,
) -> Tuple[str, int, HashDiffLookup, PathDiffLookup]:
    try:
        with open(dirdiff_state_path, "rb") as f:
            (
                dirdiff_state_hostname,
                dirdiff_state_ts,
                hashdiff_data,
                pathdiff_data,
            ) = pickle.load(f)
    except (pickle.PickleError, ValueError) as e:
        LOG.fatal(f"{dirdiff_state_path} doesn't have the appropriate format")
        sys.exit(1)

    return dirdiff_state_hostname, dirdiff_state_ts, hashdiff_data, pathdiff_data


def partition_matched_unmatched(
    hashdiff_data: HashDiffLookup, rootdirs: List[Path]
) -> Tuple[MatchResults, MatchResults]:
    matched: MatchResults = defaultdict(list)
    unmatched: MatchResults = defaultdict(list)

    # dir1/foopath1/barfile1 is matched with dir2/foopath2/barfile2 if:
    #    - "foopath1" == "foopath2" and "barfile1" == "barfile2"
    #    - hash(barfile1) == hash(barfile2).
    # Otherwise, barfile1 is unmatched.
    len_rootdirs = len(rootdirs)
    for hexdigest, filedata in hashdiff_data.items():
        for _, filelist in filedata.items():
            if len(filelist) == len_rootdirs:
                assert (
                    len({(f.time, f.machine, f.rootdir) for f in filelist})
                    == len_rootdirs
                    and len({f.hexdigest for f in filelist}) == 1
                    and len({f.position for f in filelist}) == 1
                ), ("matching error: %s" % filelist)
                matched[hexdigest].extend(filelist)
            else:
                for f in filelist:
                    unmatched[f.hexdigest].append(f)

    return matched, unmatched


def partition_unmatched_by_type(
    unmatched: MatchResults,
    pathdiff_data: PathDiffLookup,
    rootdirs: List[Path],
) -> Tuple[PathDiffLookup, List[List[DirDiffFile]], List[DirDiffFile]]:
    # We distinguish 3 cases for unmatched files:
    #  - modified file
    #  - moved/renamed file
    #  - additional file
    #
    # dir1/foopath1/barfile1 was modified if:
    #  - dir2/foopath2/barfile2 with foopath1 == foopath2 exists
    #  - hash(barfile1) != hash(barfile2)
    # dir1/foopath1/barfile1 was moved/renamed if:
    #  - there exists dir2/foopath2/barfile2 such that foopath1 != foopath2 and hash(barfile1) == hash(barfile2)
    #  - neither barfile1 nor barfile2 are modified as per the definition above
    # dir1/foopath1/barfile1 is additional if:
    #  - barfile1 is not modified as per the definition above
    #  - barfile1 is not moved/renamed as per the definition above
    len_rootdirs = len(rootdirs)
    modified: PathDiffLookup = defaultdict(list)
    moved: List[List[DirDiffFile]] = []
    additional: List[DirDiffFile] = []

    for _, filelist in unmatched.items():
        unmodified = []
        for f in filelist:
            # if all rootdirs have a file at this position -> file was modified
            if len(pathdiff_data[f.position]) == len_rootdirs:
                modified[f.position].append(f)
            else:
                unmodified.append(f)
        # any unmodified file is either moved/renamed, or isn't in all rootdirs
        # all these files have the same hexdigest, and don't necessarily have the same path
        if len(unmodified) > 0:
            # if there's one file per repo, then at least one file was moved
            if len({f.rootdir for f in unmodified}) == len_rootdirs:
                moved.append(unmodified)
            # otherwise, who knows
            else:
                additional.extend(unmodified)

    return modified, moved, additional


def print_results(
    modified: PathDiffLookup,
    moved: List[List[DirDiffFile]],
    additional: List[DirDiffFile],
    realtime: bool = True,
    ignore_modified: bool = False,
    ignore_moved: bool = False,
    ignore_additional: bool = False,
    json_output: bool = False,
) -> None:
    output_dict = {}
    if not ignore_modified:
        output_dict["modified"] = {str(k): v for k, v in modified.items()}
    if not ignore_moved:
        output_dict["moved"] = moved
    if not ignore_additional:
        output_dict["added"] = additional

    if json_output:
        print(json.dumps(output_dict, indent=2, cls=JsonEncoder))
    else:
        if not ignore_modified:
            for filepath in output_dict["modified"]:
                print("modified:", filepath)
        if not ignore_moved:
            for ren in output_dict["moved"]:
                moved_str = "\t".join([str(f.absolute(realtime=realtime)) for f in ren])
                print("moved/renamed:", moved_str)
        if not ignore_additional:
            for add in output_dict["added"]:
                print("added:", add.absolute(realtime=realtime))


def main():
    args = parse_args()

    hashdiff_data, pathdiff_data, realtime = preprocess_into_datastructs(args.dirpath)
    if args.persist_lookup_tables:
        save_state(args.dirpath, hashdiff_data, pathdiff_data)
    else:
        matched, unmatched = partition_matched_unmatched(hashdiff_data, args.dirpath)
        modified, moved, additional = partition_unmatched_by_type(
            unmatched, pathdiff_data, args.dirpath
        )
        print_results(
            modified,
            moved,
            additional,
            realtime=realtime,
            ignore_modified=args.ignore_modified,
            ignore_moved=args.ignore_moved,
            ignore_additional=args.ignore_additional,
            json_output=args.json,
        )


if __name__ == "__main__":
    main()
